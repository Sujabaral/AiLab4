{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FYWEbPlZRGrW"
      ],
      "authorship_tag": "ABX9TyPNCyG72pgawwz+WtJvWhOF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujabaral/AiLab4/blob/main/Ai_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDvuqbZ8PydY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI LABSHEET 4**\n",
        "\n",
        "Name: Suja Baral\n",
        "\n",
        "CRN: 021-387\n",
        "\n",
        "Date: 21/06/2025\n",
        "\n"
      ],
      "metadata": {
        "id": "6Vnc7yQ1Qljh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Perceptron for 2-Input Basic Gates (AND/OR)***"
      ],
      "metadata": {
        "id": "FYWEbPlZRGrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train_perceptron(X, Y, learning_rate=0.1, max_iterations=100):\n",
        "    weights = [0, 0]\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(max_iterations):  # max_iterations to avoid infinite loop\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            x = X[i]\n",
        "            y = Y[i]\n",
        "            y_pred = predict(x, weights, bias)\n",
        "            error = y - y_pred\n",
        "\n",
        "            if error != 0:\n",
        "                # Update weights and bias\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += learning_rate * error * x[j]\n",
        "                bias += learning_rate * error\n",
        "                error_found = True\n",
        "\n",
        "        if not error_found:\n",
        "            break  # Stop if no errors (converged)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Input truth table\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "# AND Gate\n",
        "print(\"---- AND Gate ----\")\n",
        "Y_and = [0, 0, 0, 1]\n",
        "w_and, b_and = train_perceptron(X, Y_and)\n",
        "print(f\"Weights: {w_and}, Bias: {b_and}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "# OR Gate\n",
        "print(\"\\n---- OR Gate ----\")\n",
        "Y_or = [0, 1, 1, 1]\n",
        "w_or, b_or = train_perceptron(X, Y_or)\n",
        "print(f\"Weights: {w_or}, Bias: {b_or}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_or, w_or, b_or)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgcX4zGCRQvE",
        "outputId": "2e16d961-0cda-47f3-e6b2-d5222396eece"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- AND Gate ----\n",
            "Weights: [0.2, 0.1], Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "---- OR Gate ----\n",
            "Weights: [0.1, 0.1], Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Perceptron for n-Input Basic Gates (AND/OR)**\n"
      ],
      "metadata": {
        "id": "a3h6jxUHR7uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train(X, Y, lr=0.1, max_loops=100):\n",
        "    weights = [0.0] * len(X[0])\n",
        "    bias = 0.0\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            y_pred = predict(X[i], weights, bias)\n",
        "            error = Y[i] - y_pred\n",
        "            if error != 0:\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += lr * error * X[i][j]\n",
        "                bias += lr * error\n",
        "                error_found = True\n",
        "        if not error_found:\n",
        "            break\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def test_accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Run for both 3-input and 4-input gates\n",
        "for n in [3, 4]:\n",
        "    print(f\"\\n==== {n}-INPUT GATES ====\")\n",
        "    X = list(product([0, 1], repeat=n))\n",
        "\n",
        "    # AND Gate\n",
        "    Y_and = [int(all(x)) for x in X]\n",
        "    w_and, b_and = train(X, Y_and)\n",
        "    print(f\"\\nAND Gate:\")\n",
        "    print(f\"Weights: {w_and}\")\n",
        "    print(f\"Bias: {b_and}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "    # OR Gate\n",
        "    Y_or = [int(any(x)) for x in X]\n",
        "    w_or, b_or = train(X, Y_or)\n",
        "    print(f\"\\nOR Gate:\")\n",
        "    print(f\"Weights: {w_or}\")\n",
        "    print(f\"Bias: {b_or}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_or, w_or, b_or)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4fDVZHSABj",
        "outputId": "e92734ac-f7c3-45b7-d5b1-6640406677c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 3-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n",
            "\n",
            "==== 4-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.4, 0.20000000000000004, 0.1, 0.1]\n",
            "Bias: -0.7999999999999999\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Perceptron for Linear Function with 3 Features**\n"
      ],
      "metadata": {
        "id": "kgOtRDVYSJjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset: y = 2x1 + 3x2 - x3 + 5\n",
        "X = []\n",
        "Y = []\n",
        "for _ in range(10):\n",
        "    x1 = random.uniform(0, 1)\n",
        "    x2 = random.uniform(0, 1)\n",
        "    x3 = random.uniform(0, 1)\n",
        "    y = 2 * x1 + 3 * x2 - 1 * x3 + 5\n",
        "    X.append([x1, x2, x3])\n",
        "    Y.append(y)\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = [0.0, 0.0, 0.0]\n",
        "bias = 0.0\n",
        "lr = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        y_true = Y[i]\n",
        "\n",
        "        # Linear output (no activation)\n",
        "        y_pred = sum([x[j] * weights[j] for j in range(3)]) + bias\n",
        "\n",
        "        # Error\n",
        "        error = y_true - y_pred\n",
        "\n",
        "        # Update weights and bias\n",
        "        for j in range(3):\n",
        "            weights[j] += lr * error * x[j]\n",
        "        bias += lr * error\n",
        "\n",
        "        total_error += error ** 2  # squared error\n",
        "\n",
        "    mse = total_error / len(X)\n",
        "    print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Final result\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCYUIG8JSQMt",
        "outputId": "5583eafc-ec4c-4810-b73a-c37c3bc15489"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 39.6985\n",
            "Epoch 2: MSE = 27.1685\n",
            "Epoch 3: MSE = 18.6450\n",
            "Epoch 4: MSE = 12.8459\n",
            "Epoch 5: MSE = 8.8995\n",
            "Epoch 6: MSE = 6.2131\n",
            "Epoch 7: MSE = 4.3837\n",
            "Epoch 8: MSE = 3.1372\n",
            "Epoch 9: MSE = 2.2873\n",
            "Epoch 10: MSE = 1.7072\n",
            "Epoch 11: MSE = 1.3107\n",
            "Epoch 12: MSE = 1.0391\n",
            "Epoch 13: MSE = 0.8526\n",
            "Epoch 14: MSE = 0.7240\n",
            "Epoch 15: MSE = 0.6350\n",
            "Epoch 16: MSE = 0.5728\n",
            "Epoch 17: MSE = 0.5289\n",
            "Epoch 18: MSE = 0.4976\n",
            "Epoch 19: MSE = 0.4749\n",
            "Epoch 20: MSE = 0.4580\n",
            "Epoch 21: MSE = 0.4452\n",
            "Epoch 22: MSE = 0.4351\n",
            "Epoch 23: MSE = 0.4269\n",
            "Epoch 24: MSE = 0.4201\n",
            "Epoch 25: MSE = 0.4142\n",
            "Epoch 26: MSE = 0.4089\n",
            "Epoch 27: MSE = 0.4041\n",
            "Epoch 28: MSE = 0.3996\n",
            "Epoch 29: MSE = 0.3954\n",
            "Epoch 30: MSE = 0.3914\n",
            "Epoch 31: MSE = 0.3875\n",
            "Epoch 32: MSE = 0.3837\n",
            "Epoch 33: MSE = 0.3800\n",
            "Epoch 34: MSE = 0.3764\n",
            "Epoch 35: MSE = 0.3728\n",
            "Epoch 36: MSE = 0.3693\n",
            "Epoch 37: MSE = 0.3659\n",
            "Epoch 38: MSE = 0.3625\n",
            "Epoch 39: MSE = 0.3592\n",
            "Epoch 40: MSE = 0.3559\n",
            "Epoch 41: MSE = 0.3526\n",
            "Epoch 42: MSE = 0.3494\n",
            "Epoch 43: MSE = 0.3462\n",
            "Epoch 44: MSE = 0.3431\n",
            "Epoch 45: MSE = 0.3400\n",
            "Epoch 46: MSE = 0.3370\n",
            "Epoch 47: MSE = 0.3339\n",
            "Epoch 48: MSE = 0.3310\n",
            "Epoch 49: MSE = 0.3280\n",
            "Epoch 50: MSE = 0.3251\n",
            "Epoch 51: MSE = 0.3222\n",
            "Epoch 52: MSE = 0.3194\n",
            "Epoch 53: MSE = 0.3166\n",
            "Epoch 54: MSE = 0.3138\n",
            "Epoch 55: MSE = 0.3110\n",
            "Epoch 56: MSE = 0.3083\n",
            "Epoch 57: MSE = 0.3056\n",
            "Epoch 58: MSE = 0.3030\n",
            "Epoch 59: MSE = 0.3004\n",
            "Epoch 60: MSE = 0.2978\n",
            "Epoch 61: MSE = 0.2952\n",
            "Epoch 62: MSE = 0.2927\n",
            "Epoch 63: MSE = 0.2902\n",
            "Epoch 64: MSE = 0.2877\n",
            "Epoch 65: MSE = 0.2853\n",
            "Epoch 66: MSE = 0.2829\n",
            "Epoch 67: MSE = 0.2805\n",
            "Epoch 68: MSE = 0.2781\n",
            "Epoch 69: MSE = 0.2758\n",
            "Epoch 70: MSE = 0.2735\n",
            "Epoch 71: MSE = 0.2712\n",
            "Epoch 72: MSE = 0.2689\n",
            "Epoch 73: MSE = 0.2667\n",
            "Epoch 74: MSE = 0.2645\n",
            "Epoch 75: MSE = 0.2623\n",
            "Epoch 76: MSE = 0.2601\n",
            "Epoch 77: MSE = 0.2580\n",
            "Epoch 78: MSE = 0.2559\n",
            "Epoch 79: MSE = 0.2538\n",
            "Epoch 80: MSE = 0.2517\n",
            "Epoch 81: MSE = 0.2497\n",
            "Epoch 82: MSE = 0.2477\n",
            "Epoch 83: MSE = 0.2457\n",
            "Epoch 84: MSE = 0.2437\n",
            "Epoch 85: MSE = 0.2417\n",
            "Epoch 86: MSE = 0.2398\n",
            "Epoch 87: MSE = 0.2379\n",
            "Epoch 88: MSE = 0.2360\n",
            "Epoch 89: MSE = 0.2341\n",
            "Epoch 90: MSE = 0.2323\n",
            "Epoch 91: MSE = 0.2304\n",
            "Epoch 92: MSE = 0.2286\n",
            "Epoch 93: MSE = 0.2268\n",
            "Epoch 94: MSE = 0.2251\n",
            "Epoch 95: MSE = 0.2233\n",
            "Epoch 96: MSE = 0.2216\n",
            "Epoch 97: MSE = 0.2198\n",
            "Epoch 98: MSE = 0.2181\n",
            "Epoch 99: MSE = 0.2165\n",
            "Epoch 100: MSE = 0.2148\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weights: [2.3314351643795996, 1.9198744913272938, 1.3333819309270576]\n",
            "Bias: 3.843535192480406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Perceptron for Linear Function with n Features**\n"
      ],
      "metadata": {
        "id": "UzsRdpKlSWL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_perceptron_linear(X, Y, lr=0.01, epochs=100):\n",
        "    n_features = len(X[0])\n",
        "    weights = [0.0] * n_features\n",
        "    bias = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = sum(X[i][j] * weights[j] for j in range(n_features)) + bias\n",
        "            error = Y[i] - y_pred\n",
        "            # Update weights and bias\n",
        "            for j in range(n_features):\n",
        "                weights[j] += lr * error * X[i][j]\n",
        "            bias += lr * error\n",
        "            total_error += error ** 2\n",
        "        mse = total_error / len(X)\n",
        "        print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "def generate_dataset(n_features, n_samples=10):\n",
        "    # Generate true random weights in [-1,1]\n",
        "    true_weights = [random.uniform(-1, 1) for _ in range(n_features)]\n",
        "    true_bias = 5  # fixed bias\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(n_samples):\n",
        "        features = [random.uniform(0, 1) for _ in range(n_features)]\n",
        "        y = sum(features[i] * true_weights[i] for i in range(n_features)) + true_bias\n",
        "        X.append(features)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y, true_weights, true_bias\n",
        "\n",
        "\n",
        "# Test for n=4 and n=5\n",
        "for n in [4, 5]:\n",
        "    print(f\"\\nTraining for n={n} features:\")\n",
        "    X, Y, true_w, true_b = generate_dataset(n)\n",
        "    print(f\"True weights: {true_w}\")\n",
        "    print(f\"True bias: {true_b}\")\n",
        "    learned_weights, learned_bias = train_perceptron_linear(X, Y)\n",
        "    print(f\"Learned weights: {learned_weights}\")\n",
        "    print(f\"Learned bias: {learned_bias}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLpPC9DGS2BR",
        "outputId": "0ef439c4-88b1-4f61-f849-2a24976b5125"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training for n=4 features:\n",
            "True weights: [-0.9027570038311463, 0.07880228908367859, 0.09675964575536322, -0.28667820256570575]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 15.6939\n",
            "Epoch 2: MSE = 9.7734\n",
            "Epoch 3: MSE = 6.1173\n",
            "Epoch 4: MSE = 3.8598\n",
            "Epoch 5: MSE = 2.4659\n",
            "Epoch 6: MSE = 1.6053\n",
            "Epoch 7: MSE = 1.0739\n",
            "Epoch 8: MSE = 0.7458\n",
            "Epoch 9: MSE = 0.5431\n",
            "Epoch 10: MSE = 0.4178\n",
            "Epoch 11: MSE = 0.3402\n",
            "Epoch 12: MSE = 0.2921\n",
            "Epoch 13: MSE = 0.2621\n",
            "Epoch 14: MSE = 0.2433\n",
            "Epoch 15: MSE = 0.2314\n",
            "Epoch 16: MSE = 0.2237\n",
            "Epoch 17: MSE = 0.2186\n",
            "Epoch 18: MSE = 0.2151\n",
            "Epoch 19: MSE = 0.2126\n",
            "Epoch 20: MSE = 0.2107\n",
            "Epoch 21: MSE = 0.2092\n",
            "Epoch 22: MSE = 0.2079\n",
            "Epoch 23: MSE = 0.2067\n",
            "Epoch 24: MSE = 0.2056\n",
            "Epoch 25: MSE = 0.2046\n",
            "Epoch 26: MSE = 0.2036\n",
            "Epoch 27: MSE = 0.2027\n",
            "Epoch 28: MSE = 0.2017\n",
            "Epoch 29: MSE = 0.2008\n",
            "Epoch 30: MSE = 0.1999\n",
            "Epoch 31: MSE = 0.1990\n",
            "Epoch 32: MSE = 0.1981\n",
            "Epoch 33: MSE = 0.1972\n",
            "Epoch 34: MSE = 0.1963\n",
            "Epoch 35: MSE = 0.1955\n",
            "Epoch 36: MSE = 0.1946\n",
            "Epoch 37: MSE = 0.1938\n",
            "Epoch 38: MSE = 0.1929\n",
            "Epoch 39: MSE = 0.1921\n",
            "Epoch 40: MSE = 0.1913\n",
            "Epoch 41: MSE = 0.1905\n",
            "Epoch 42: MSE = 0.1897\n",
            "Epoch 43: MSE = 0.1889\n",
            "Epoch 44: MSE = 0.1881\n",
            "Epoch 45: MSE = 0.1873\n",
            "Epoch 46: MSE = 0.1866\n",
            "Epoch 47: MSE = 0.1858\n",
            "Epoch 48: MSE = 0.1850\n",
            "Epoch 49: MSE = 0.1843\n",
            "Epoch 50: MSE = 0.1835\n",
            "Epoch 51: MSE = 0.1828\n",
            "Epoch 52: MSE = 0.1821\n",
            "Epoch 53: MSE = 0.1814\n",
            "Epoch 54: MSE = 0.1806\n",
            "Epoch 55: MSE = 0.1799\n",
            "Epoch 56: MSE = 0.1792\n",
            "Epoch 57: MSE = 0.1785\n",
            "Epoch 58: MSE = 0.1779\n",
            "Epoch 59: MSE = 0.1772\n",
            "Epoch 60: MSE = 0.1765\n",
            "Epoch 61: MSE = 0.1758\n",
            "Epoch 62: MSE = 0.1752\n",
            "Epoch 63: MSE = 0.1745\n",
            "Epoch 64: MSE = 0.1738\n",
            "Epoch 65: MSE = 0.1732\n",
            "Epoch 66: MSE = 0.1726\n",
            "Epoch 67: MSE = 0.1719\n",
            "Epoch 68: MSE = 0.1713\n",
            "Epoch 69: MSE = 0.1707\n",
            "Epoch 70: MSE = 0.1700\n",
            "Epoch 71: MSE = 0.1694\n",
            "Epoch 72: MSE = 0.1688\n",
            "Epoch 73: MSE = 0.1682\n",
            "Epoch 74: MSE = 0.1676\n",
            "Epoch 75: MSE = 0.1670\n",
            "Epoch 76: MSE = 0.1664\n",
            "Epoch 77: MSE = 0.1658\n",
            "Epoch 78: MSE = 0.1652\n",
            "Epoch 79: MSE = 0.1647\n",
            "Epoch 80: MSE = 0.1641\n",
            "Epoch 81: MSE = 0.1635\n",
            "Epoch 82: MSE = 0.1630\n",
            "Epoch 83: MSE = 0.1624\n",
            "Epoch 84: MSE = 0.1618\n",
            "Epoch 85: MSE = 0.1613\n",
            "Epoch 86: MSE = 0.1607\n",
            "Epoch 87: MSE = 0.1602\n",
            "Epoch 88: MSE = 0.1596\n",
            "Epoch 89: MSE = 0.1591\n",
            "Epoch 90: MSE = 0.1586\n",
            "Epoch 91: MSE = 0.1580\n",
            "Epoch 92: MSE = 0.1575\n",
            "Epoch 93: MSE = 0.1570\n",
            "Epoch 94: MSE = 0.1565\n",
            "Epoch 95: MSE = 0.1560\n",
            "Epoch 96: MSE = 0.1554\n",
            "Epoch 97: MSE = 0.1549\n",
            "Epoch 98: MSE = 0.1544\n",
            "Epoch 99: MSE = 0.1539\n",
            "Epoch 100: MSE = 0.1534\n",
            "Learned weights: [0.8152700006894145, 0.7287967694069968, 1.2209862165904921, 0.9922418841710369]\n",
            "Learned bias: 2.171959939348618\n",
            "\n",
            "Training for n=5 features:\n",
            "True weights: [0.12810150209919602, -0.7137444265812427, 0.9877795962351335, -0.14034106522527212, -0.4572419728631121]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 21.5606\n",
            "Epoch 2: MSE = 14.7119\n",
            "Epoch 3: MSE = 10.0645\n",
            "Epoch 4: MSE = 6.9099\n",
            "Epoch 5: MSE = 4.7676\n",
            "Epoch 6: MSE = 3.3120\n",
            "Epoch 7: MSE = 2.3221\n",
            "Epoch 8: MSE = 1.6483\n",
            "Epoch 9: MSE = 1.1890\n",
            "Epoch 10: MSE = 0.8752\n",
            "Epoch 11: MSE = 0.6604\n",
            "Epoch 12: MSE = 0.5127\n",
            "Epoch 13: MSE = 0.4108\n",
            "Epoch 14: MSE = 0.3399\n",
            "Epoch 15: MSE = 0.2902\n",
            "Epoch 16: MSE = 0.2550\n",
            "Epoch 17: MSE = 0.2296\n",
            "Epoch 18: MSE = 0.2111\n",
            "Epoch 19: MSE = 0.1972\n",
            "Epoch 20: MSE = 0.1865\n",
            "Epoch 21: MSE = 0.1780\n",
            "Epoch 22: MSE = 0.1712\n",
            "Epoch 23: MSE = 0.1655\n",
            "Epoch 24: MSE = 0.1605\n",
            "Epoch 25: MSE = 0.1562\n",
            "Epoch 26: MSE = 0.1523\n",
            "Epoch 27: MSE = 0.1487\n",
            "Epoch 28: MSE = 0.1454\n",
            "Epoch 29: MSE = 0.1423\n",
            "Epoch 30: MSE = 0.1394\n",
            "Epoch 31: MSE = 0.1366\n",
            "Epoch 32: MSE = 0.1340\n",
            "Epoch 33: MSE = 0.1315\n",
            "Epoch 34: MSE = 0.1291\n",
            "Epoch 35: MSE = 0.1268\n",
            "Epoch 36: MSE = 0.1246\n",
            "Epoch 37: MSE = 0.1224\n",
            "Epoch 38: MSE = 0.1204\n",
            "Epoch 39: MSE = 0.1184\n",
            "Epoch 40: MSE = 0.1165\n",
            "Epoch 41: MSE = 0.1147\n",
            "Epoch 42: MSE = 0.1129\n",
            "Epoch 43: MSE = 0.1112\n",
            "Epoch 44: MSE = 0.1096\n",
            "Epoch 45: MSE = 0.1080\n",
            "Epoch 46: MSE = 0.1065\n",
            "Epoch 47: MSE = 0.1050\n",
            "Epoch 48: MSE = 0.1036\n",
            "Epoch 49: MSE = 0.1022\n",
            "Epoch 50: MSE = 0.1009\n",
            "Epoch 51: MSE = 0.0996\n",
            "Epoch 52: MSE = 0.0984\n",
            "Epoch 53: MSE = 0.0972\n",
            "Epoch 54: MSE = 0.0961\n",
            "Epoch 55: MSE = 0.0950\n",
            "Epoch 56: MSE = 0.0939\n",
            "Epoch 57: MSE = 0.0929\n",
            "Epoch 58: MSE = 0.0919\n",
            "Epoch 59: MSE = 0.0909\n",
            "Epoch 60: MSE = 0.0900\n",
            "Epoch 61: MSE = 0.0891\n",
            "Epoch 62: MSE = 0.0882\n",
            "Epoch 63: MSE = 0.0874\n",
            "Epoch 64: MSE = 0.0865\n",
            "Epoch 65: MSE = 0.0858\n",
            "Epoch 66: MSE = 0.0850\n",
            "Epoch 67: MSE = 0.0843\n",
            "Epoch 68: MSE = 0.0835\n",
            "Epoch 69: MSE = 0.0829\n",
            "Epoch 70: MSE = 0.0822\n",
            "Epoch 71: MSE = 0.0815\n",
            "Epoch 72: MSE = 0.0809\n",
            "Epoch 73: MSE = 0.0803\n",
            "Epoch 74: MSE = 0.0797\n",
            "Epoch 75: MSE = 0.0792\n",
            "Epoch 76: MSE = 0.0786\n",
            "Epoch 77: MSE = 0.0781\n",
            "Epoch 78: MSE = 0.0776\n",
            "Epoch 79: MSE = 0.0771\n",
            "Epoch 80: MSE = 0.0766\n",
            "Epoch 81: MSE = 0.0761\n",
            "Epoch 82: MSE = 0.0756\n",
            "Epoch 83: MSE = 0.0752\n",
            "Epoch 84: MSE = 0.0748\n",
            "Epoch 85: MSE = 0.0744\n",
            "Epoch 86: MSE = 0.0740\n",
            "Epoch 87: MSE = 0.0736\n",
            "Epoch 88: MSE = 0.0732\n",
            "Epoch 89: MSE = 0.0728\n",
            "Epoch 90: MSE = 0.0724\n",
            "Epoch 91: MSE = 0.0721\n",
            "Epoch 92: MSE = 0.0718\n",
            "Epoch 93: MSE = 0.0714\n",
            "Epoch 94: MSE = 0.0711\n",
            "Epoch 95: MSE = 0.0708\n",
            "Epoch 96: MSE = 0.0705\n",
            "Epoch 97: MSE = 0.0702\n",
            "Epoch 98: MSE = 0.0699\n",
            "Epoch 99: MSE = 0.0696\n",
            "Epoch 100: MSE = 0.0693\n",
            "Learned weights: [0.9048958189865346, 0.6515219906758104, 1.643901595587103, 1.4154886959046526, 0.5058283187965237]\n",
            "Learned bias: 2.776493884926669\n"
          ]
        }
      ]
    }
  ]
}